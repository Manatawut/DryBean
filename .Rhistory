plot(
tuned.linear,
df.fil,
ram ~ battery_power,
grid = 500,
symbolPalette = c("red", "green", "blue","white"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
# code here
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=0.1
)
summary(model.lm)
plot(
model.lm,
df.fil,
ram ~ battery_power,
grid = 200,
symbolPalette = c("red", "green", "blue","white"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
# code here
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=1
)
summary(model.lm)
plot(
model.lm,
df.fil,
ram ~ battery_power,
grid = 200,
symbolPalette = c("red", "green", "blue","white"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
# code here
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=1
)
summary(model.lm)
plot(
model.lm,
df.fil,
ram ~ battery_power,
grid = 200,
symbolPalette = c("red", "green", "blue","yellow"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
# code here
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=1
)
summary(model.lm)
plot(
model.lm,
df.fil,
ram ~ battery_power,
grid = 200,
symbolPalette = c("red", "green", "blue","yellow"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
summary(model.lm)
plot(
model.lm,
df.fil,
ram ~ battery_power,
grid = 200,
symbolPalette = c("red", "green", "blue","yellow"),
svSymbol = 24,
dataSymbol = 20,
col = c("lightblue", "pink", "orange","red")
)
# code here
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=1
)
model.lm <-
svm(
price_range ~ battery_power + ram ,
kernel="linear",
cross=10,
scale = F,
data=df.fil,
cost=0.1
)
summary(model.lm)
# code here
set.seed(1234)
tune.results <-
tune(
svm,
price_range ~ battery_power + ram ,
data = df.fil ,
kernel = 'linear',
tunecontrol = tune.control(cross = 10),
ranges = list(cost = c(0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100))
)
summary(tune.results)
plot(tune.results)
tuned.linear <- tune.results$best.model
summary(tuned.linear)
library(reticulate)
library(keras)
library(tensorflow)
library(ggplot2)
library(dplyr)
df <- read.csv("cancer.csv")
df$diag <- factor(df$diag)
df.num <- dplyr::select(df,-diag)
maxs <- apply(df.num, 2, max)
mins <- apply(df.num, 2, min)
maxs
mins
df.scaled <- df.num %>% scale(center = mins, scale = maxs - mins) %>% data.frame
df.scaled$diag <- as.numeric(df$diag)-1
head(df.scaled)
set.seed(101)
tr = caTools::sample.split(df.scaled$diag, SplitRatio = 0.70)
df.train = subset(df.scaled, tr)
df.validate = subset(df.scaled, !tr)
#Your code here
train.labels <- to_categorical(df.train$diag)
head(train.labels)
colSums(train.labels)
validate.labels <- to_categorical(df.validate$diag)
head(validate.labels)
colSums(validate.labels)
#Your code here
# Initialize a sequential model
model <- keras_model_sequential()
# Add layers to the model 1 layer = 1 layer_dense()
model %>%
layer_dense(units = 10, activation = 'tanh', input_shape = 4) %>%
layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 3, activation = 'tanh') %>%
layer_dense(units = 3, activation = 'softmax')
#Your code here
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(lr = 0.05),
metrics = 'accuracy'
)
#Your code here
train.mat <- select(df.train, -diag) %>% as.matrix
validate.mat <- select(df.validate, -diag) %>% as.matrix
history <- model %>% fit(
train.mat,
train.labels,
epochs = 150,
batch_size = 5,
validation_data = list(validate.mat, validate.labels )
)
#Your code here
train.mat <- select(df.train, -diag) %>% as.matrix
validate.mat <- select(df.validate, -diag) %>% as.matrix
history <- model %>% fit(
train.mat,
train.labels,
epochs = 150,
batch_size = 5,
validation_data = 0.3
)
library(reticulate)
library(keras)
library(tensorflow)
library(ggplot2)
library(dplyr)
df <- read.csv("cancer.csv")
df$diag <- factor(df$diag)
df.num <- dplyr::select(df,-diag)
maxs <- apply(df.num, 2, max)
mins <- apply(df.num, 2, min)
maxs
mins
df.scaled <- df.num %>% scale(center = mins, scale = maxs - mins) %>% data.frame
df.scaled$diag <- as.numeric(df$diag)-1
head(df.scaled)
set.seed(101)
tr = caTools::sample.split(df.scaled$diag, SplitRatio = 0.70)
df.train = subset(df.scaled, tr)
df.validate = subset(df.scaled, !tr)
#Your code here
train.labels <- to_categorical(df.train$diag)
head(train.labels)
colSums(train.labels)
validate.labels <- to_categorical(df.validate$diag)
head(validate.labels)
colSums(validate.labels)
#Your code here
# Initialize a sequential model
model <- keras_model_sequential()
# Add layers to the model 1 layer = 1 layer_dense()
model %>%
layer_dense(units = 10, activation = 'tanh', input_shape = 30) %>%
layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 3, activation = 'tanh') %>%
layer_dense(units = 2, activation = 'softmax')
#Your code here
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(lr = 0.05),
metrics = 'accuracy'
)
#Your code here
train.mat <- select(df.train, -diag) %>% as.matrix
validate.mat <- select(df.validate, -diag) %>% as.matrix
history <- model %>% fit(
train.mat,
train.labels,
epochs = 150,
batch_size = 5,
validation_data = list(validate.mat, validate.labels )
)
#Your code here
# Initialize a sequential model
model <- keras_model_sequential()
# Add layers to the model 1 layer = 1 layer_dense()
model %>%
layer_dense(units = 10, activation = 'tanh', input_shape = 4) %>%
layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 3, activation = 'tanh') %>%
layer_dense(units = 2, activation = 'softmax')
#Your code here
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(lr = 0.05),
metrics = 'accuracy'
)
#Your code here
train.mat <- select(df.train, -diag) %>% as.matrix
validate.mat <- select(df.validate, -diag) %>% as.matrix
history <- model %>% fit(
train.mat,
train.labels,
epochs = 150,
batch_size = 5,
validation_data = list(validate.mat, validate.labels )
)
#Your code here
# Initialize a sequential model
model <- keras_model_sequential()
# Add layers to the model 1 layer = 1 layer_dense()
model %>%
layer_dense(units = 10, activation = 'tanh', input_shape = 30) %>%
layer_dense(units = 5, activation = 'tanh') %>%
layer_dense(units = 3, activation = 'tanh') %>%
layer_dense(units = 2, activation = 'softmax')
#Your code here
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(lr = 0.05),
metrics = 'accuracy'
)
#Your code here
train.mat <- select(df.train, -diag) %>% as.matrix
validate.mat <- select(df.validate, -diag) %>% as.matrix
history <- model %>% fit(
train.mat,
train.labels,
epochs = 150,
batch_size = 5,
validation_data = list(validate.mat, validate.labels )
)
#Your code here
plot(history)
dat <-
data.frame(
epoch = 1:150,
loss = history$metrics$loss,
val_loss = history$metrics$val_loss
)
dat_gat <-
tidyr::gather(dat, 'loss', 'val_loss', key = 'type', value = 'loss_val')
pl <- ggplot(aes(x = epoch, y = loss_val), data = dat_gat) +
geom_line(aes(color = type)) +
theme_bw()
pl
library(dplyr)
# Your code
df <- read.csv("divorce.csv")
dim(df)
# Your code
df_mean <- dplyr::select(df, -Divorce)
mean <- apply(df_mean, 2, mean)
mean
# Your code
df$Divorce <- factor(df$Divorce)
summary(df$Divorce)
# Your code
df$Divorce <- factor(df$Divorce)
library(e1071)
# Your code
model.svm.linear <- svm(Divorce ~ .,
kernel = "linear",
cross = 5,
scale = F,
data = df,
cost = 1,
gamma = 1
)
model.svm.linear$accuracies %>% mean
library(e1071)
# Your code
model.svm.rbf <- svm(Divorce ~ .,
kernel = "radial",
cross = 5,
scale = F,
data = df,
cost = 1,
gamma = 1
)
model.svm.rbf$accuracies %>% mean
library(RWeka)
# Your code
model.tree <- J48(Divorce ~ .,
data = df,
control = Weka_control(R = F)
)
summary(model.tree)
library(RWeka)
# Your code
model.tree <- J48(Divorce ~ .,
data = df,
control = Weka_control(R = F)
)
# Your code
plot(model.tree)
# Your code
evaluate_Weka_classifier(model.tree,
numFolds = 5,
complexity = T,
class = T,
seed = 1234
)
set.seed(1234)
# Your code
train.id <- caTools::sample.split(df$Divorce, SplitRatio = 0.70)
df.train <- subset(df, train.id)
df.test <- subset(df,!train.id)
library(neuralnet)
# Your code
model.ann <- neuralnet(Divorce ~ .,
data = df.train,
hidden = c(60, 30, 15, 5, 3),
linear.output = FALSE,
act.fct = "tanh"
)
pred <- predict(model.ann, df.test)
conf.mat <- table(df.test$Divorce, apply(pred, 1, which.max))
conf.mat
# Your code
ACC <-  (conf.mat[1,1]+conf.mat[2,2])/sum(conf.mat)
ACC
Sensitivity <-  conf.mat[2,2] / (conf.mat[2,1]+conf.mat[2,2])
Sensitivity
Specificity <-  conf.mat[1,1] / (conf.mat[1,2]+conf.mat[1,1])
Specificity
install.packages("ElemStatLearn")
library(ElemStatLearn)
setwd("D:/01418566 Statistical Data Science/Project/DryBean")
library(dplyr)
#If using all 4 predictor variables
set.seed(1234)
cl <- kmeans(iris[, 1:4], centers = 3) #centers equivalent to k
cl
table(cl$cluster, iris$Species)
library(cluster)
clusplot(iris, cl$cluster, color=T, shade=T, labels=0,lines=0)
cl$tot.withinss
Ks <- 2:25
Tots <- sapply(Ks, function(x){
kmeans(iris[,1:4], centers = x)$tot.withinss
})
Tots
library(ggplot2)
library(plotly)
pl <- ggplot(data.frame(k=Ks, Tots=Tots))+geom_point(aes(k,Tots))+geom_line(aes(k,Tots))
ggplotly(pl)
library(dplyr)
df <- select(iris,Petal.Length,Petal.Width)
cl <- kmeans(df, centers=3)
pl <- ggplot()+geom_point(data=df, aes(Petal.Length, Petal.Width), color=as.factor(cl$cluster), size=3, alpha=.5)+
geom_point(data=as.data.frame(cl$centers), aes(Petal.Length, Petal.Width), color="blue", pch="+", size=5)+
theme_bw()
ggplotly(pl)
Ks <- 2:25
Tots <- sapply(Ks, function(x){
kmeans(df, centers = x)$tot.withinss
})
pl <- ggplot(data.frame(k=Ks, Tots=Tots))+geom_point(aes(k,Tots))+geom_line(aes(k,Tots))
ggplotly(pl)
library(flexclust)
params <- kccaFamily(which="kmeans", dist="distEuclidean")
#which = "kmeans", "kmedians", "angle", "jaccard",or "ejaccard")
#dist = "distEuclidean", "distManhattan", "distMinkowski", "distMax", "distCanberra", "distCor", "distAngle",or "distJaccard"
cl <- kcca(df, family=params, k=3)
pl <- ggplot()+geom_point(aes(Petal.Length, Petal.Width, color=as.factor(cl@cluster)), data=df, size=3, alpha=.5)+
geom_point(aes(Petal.Length, Petal.Width),data=as.data.frame(cl@centers),shape="+",size=5)+
theme_bw()
ggplotly(pl)
shiny::runApp()
runApp()
setwd("D:/01418566 Statistical Data Science/Project/DryBean")
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
